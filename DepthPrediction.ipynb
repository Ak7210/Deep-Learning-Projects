{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10131214,"sourceType":"datasetVersion","datasetId":6252594}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport os\n# from torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:08.347312Z","iopub.execute_input":"2024-12-08T13:37:08.348272Z","iopub.status.idle":"2024-12-08T13:37:12.794971Z","shell.execute_reply.started":"2024-12-08T13:37:08.348220Z","shell.execute_reply":"2024-12-08T13:37:12.794030Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ignoring the warning messages\nimport warnings\nfrom IPython.display import display\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.796425Z","iopub.execute_input":"2024-12-08T13:37:12.796808Z","iopub.status.idle":"2024-12-08T13:37:12.800979Z","shell.execute_reply.started":"2024-12-08T13:37:12.796780Z","shell.execute_reply":"2024-12-08T13:37:12.800133Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class TrainValDataset(Dataset):\n    def __init__(self, image_data_path,depth_image_path, image_transform=None, depth_transform=None):\n        self.image_data_path = image_data_path\n        self.depth_image_path = depth_image_path\n        self.image_transform = image_transform\n        self.depth_transform = depth_transform\n        self.images_list = os.listdir(image_data_path) #[ images_ png]\n        # self.depth_images_list = os.listdir(depth_image_path)\n\n    def __len__(self):\n        return len(self.images_list)\n\n    def __getitem__(self, idx):\n\n        noraml_img = os.path.join(self.image_data_path, self.images_list[idx])\n        # find the same image in the depth image folder which is in the same as the images_list[idx]\n        # if self.images_list[idx] in self.depth_images_list:\n        depth_img = os.path.join(self.depth_image_path, self.images_list[idx])\n\n        # if normal_img is not None and depth_img is not None:\n\n            # depth_img = os.path.join(self.depth_image_path, self.depth_images_list[idx])\n\n        normal_img = Image.open(noraml_img).convert('RGB') # RGB is for 3 channel image\n        depth_image = Image.open(depth_img).convert('L') # L is for grayscale\n\n\n        if self.image_transform:\n            normal_img = self.image_transform(normal_img)\n\n        if self.depth_transform:\n            depth_image = self.depth_transform(depth_image)\n        \n        # depth_image = (depth_image -torch.min(depth_image))/(torch.max(depth_image)-torch.min(depth_image))\n            \n        # else:\n        #     raise ValueError(\"Image not found in Depth Image folder {images_list[idx]}\")\n\n        return normal_img, depth_image\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.802027Z","iopub.execute_input":"2024-12-08T13:37:12.802378Z","iopub.status.idle":"2024-12-08T13:37:12.814375Z","shell.execute_reply.started":"2024-12-08T13:37:12.802340Z","shell.execute_reply":"2024-12-08T13:37:12.813644Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_path, test_transform=None):\n        self.test_path = test_path\n        self.test_transform = test_transform\n        self.images_list = os.listdir(test_path)\n\n    def __len__(self):       \n        return len(self.images_list)\n\n    def __getitem__(self, idx):\n        name_image = self.images_list[idx]\n        img_name = os.path.join(self.test_path, name_image)\n        test_image = Image.open(img_name).convert('RGB') # RGB is for 3 channel image\n\n        if self.test_transform:\n            test_image = self.test_transform(test_image)\n\n        return test_image,name_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.815288Z","iopub.execute_input":"2024-12-08T13:37:12.815532Z","iopub.status.idle":"2024-12-08T13:37:12.828612Z","shell.execute_reply.started":"2024-12-08T13:37:12.815499Z","shell.execute_reply":"2024-12-08T13:37:12.827769Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Augmentaion for the norma images and depth images\nnoraml_image_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    # transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndepth_image_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    # transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.5], std=[0.5])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.830287Z","iopub.execute_input":"2024-12-08T13:37:12.830533Z","iopub.status.idle":"2024-12-08T13:37:12.844924Z","shell.execute_reply.started":"2024-12-08T13:37:12.830509Z","shell.execute_reply":"2024-12-08T13:37:12.844095Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"normal_train_img_pth = '/kaggle/input/dlp-week-10/competition-data/competition-data/training/images'\ndepth_train_img_pth = '/kaggle/input/dlp-week-10/competition-data/competition-data/training/depths'\n\n# VAlidadion path\nnormal_val_img_pth = '/kaggle/input/dlp-week-10/competition-data/competition-data/validation/images'\ndepth_val_img_pth = '/kaggle/input/dlp-week-10/competition-data/competition-data/validation/depths'\n\n#test path\nnormal_test_img_pth = '/kaggle/input/dlp-week-10/competition-data/competition-data/testing/images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.845981Z","iopub.execute_input":"2024-12-08T13:37:12.846301Z","iopub.status.idle":"2024-12-08T13:37:12.855806Z","shell.execute_reply.started":"2024-12-08T13:37:12.846266Z","shell.execute_reply":"2024-12-08T13:37:12.854965Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#call the custom dataset\ndataset = TrainValDataset(normal_train_img_pth, depth_train_img_pth, noraml_image_transform, depth_image_transform)\n# Validation dataset\nval_dataset = TrainValDataset(normal_val_img_pth, depth_val_img_pth, noraml_image_transform, depth_image_transform)\n# Test dataset\ntest_dataset = TestDataset(normal_test_img_pth, noraml_image_transform )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.856809Z","iopub.execute_input":"2024-12-08T13:37:12.857080Z","iopub.status.idle":"2024-12-08T13:37:12.975617Z","shell.execute_reply.started":"2024-12-08T13:37:12.857056Z","shell.execute_reply":"2024-12-08T13:37:12.974654Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"batch_size = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.976955Z","iopub.execute_input":"2024-12-08T13:37:12.977561Z","iopub.status.idle":"2024-12-08T13:37:12.981788Z","shell.execute_reply.started":"2024-12-08T13:37:12.977509Z","shell.execute_reply":"2024-12-08T13:37:12.980834Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Dataloader\ntrain_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n# Validation dataloader\nval_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n# Test Dataloader\ntest_loader = DataLoader(dataset = test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.982786Z","iopub.execute_input":"2024-12-08T13:37:12.983083Z","iopub.status.idle":"2024-12-08T13:37:12.996616Z","shell.execute_reply.started":"2024-12-08T13:37:12.983058Z","shell.execute_reply":"2024-12-08T13:37:12.995811Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# for idx, (imag, names) in enumerate(test_loader):\n#     count = 0\n#     if count < 50:\n#         print(names)\n#         count +=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:12.997625Z","iopub.execute_input":"2024-12-08T13:37:12.997854Z","iopub.status.idle":"2024-12-08T13:37:13.008814Z","shell.execute_reply.started":"2024-12-08T13:37:12.997828Z","shell.execute_reply":"2024-12-08T13:37:13.007894Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# class CustomDepthEstimationModel(nn.Module):\n#     def __init__(self):\n#         super(CustomDepthEstimationModel, self).__init__()\n#         #channels: 3->64,\n#         self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n#         self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n#         #channels: 64->128\n#         self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n#         self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n#         #channels: 128->256\n#         self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv6 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n#         self.maxpool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n#         #channels: 256->512\n#         self.conv7 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv8 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n#         self.maxpool4 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n#         #channels:512->1024\n#         self.conv9 = nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv10 = nn.Conv2d(in_channels = 1024, out_channels = 1024, kernel_size = 3, stride = 1, padding = 1)\n#         self.upconv1 = nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size = 2, stride = 2)\n#         #channels:1024->512\n#         self.conv11 = nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv12 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n#         self.upconv2 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 2, stride = 2)\n#         #channels: 512->256\n#         self.conv13 = nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv14 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n#         self.upconv3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 2, stride = 2)\n#         #channels:256->128\n#         self.conv15 = nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv16 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n#         self.upconv4 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 2, stride = 2)\n#         #channels:128->64\n#         self.conv17 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n#         self.conv18 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n#         #channels:64->1\n#         self.conv19 = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 1, stride = 1, padding = 0)\n\n#         #relu: for non-linearity relu(x)=max(0,x)\n#         self.relu = nn.ReLU()\n#         #group_normalisation\n#         self.gn1 = nn.GroupNorm(16, 64)\n#         self.gn2 = nn.GroupNorm(16, 128)\n#         self.gn3 = nn.GroupNorm(16, 256)\n#         self.gn4 = nn.GroupNorm(16, 512)\n#         self.gn5 = nn.GroupNorm(16, 1024)\n#         #drop out layers; set the fraction of neurons to 0\n#         self.dropout = nn.Dropout(0.5)\n#         self.dropout1 = nn.Dropout(0.25)\n\n#     def forward(self, x):\n#         x = self.conv1(x)\n#         x = self.gn1(x)\n#         x = self.relu(x)\n#         x = self.conv2(x)\n#         x = self.gn1(x)\n#         x = self.relu(x)\n#         out1 = x\n#         x = self.maxpool1(x)\n#         x = self.dropout1(x)\n#         x = self.conv3(x)\n#         x = self.gn2(x)\n#         x = self.relu(x)\n#         x = self.conv4(x)\n#         x = self.gn2(x)\n#         x = self.relu(x)\n#         out2 = x\n#         x = self.maxpool2(x)\n#         x = self.dropout(x)\n#         x = self.conv5(x)\n#         x = self.gn3(x)\n#         x = self.relu(x)\n#         x = self.conv6(x)\n#         x = self.gn3(x)\n#         x = self.relu(x)\n#         out3 = x\n#         x = self.maxpool3(x)\n#         x = self.dropout(x)\n#         x = self.conv7(x)\n#         x = self.gn4(x)\n#         x = self.relu(x)\n#         x = self.conv8(x)\n#         x = self.gn4(x)\n#         x = self.relu(x)\n#         out4 = x\n#         x = self.maxpool4(x)\n#         x = self.dropout(x)\n#         x = self.conv9(x)\n#         x = self.gn5(x)\n#         x = self.relu(x)\n#         x = self.conv10(x)\n#         x = self.gn5(x)\n#         x = self.relu(x)\n#         x = self.upconv1(x)\n#         x = torch.cat((x, out4), 1)\n#         x = self.dropout(x)\n#         x = self.conv11(x)\n#         x = self.gn4(x)\n#         x = self.relu(x)\n#         x = self.conv12(x)\n#         x = self.gn4(x)\n#         x = self.relu(x)\n#         x = self.upconv2(x)\n#         x = torch.cat((x, out3), 1)\n#         x = self.dropout(x)\n#         x = self.conv13(x)\n#         x = self.gn3(x)\n#         x = self.relu(x)\n#         x = self.conv14(x)\n#         x = self.gn3(x)\n#         x = self.relu(x)\n#         x = self.upconv3(x)\n#         x = torch.cat((x, out2), 1)\n#         x = self.dropout(x)\n#         x = self.conv15(x)\n#         x = self.gn2(x)\n#         x = self.relu(x)\n#         x = self.conv16(x)\n#         x = self.gn2(x)\n#         x = self.relu(x)\n#         x = self.upconv4(x)\n#         x = torch.cat((x, out1), 1)\n#         x = self.dropout(x)\n#         x = self.conv17(x)\n#         x = self.gn1(x)\n#         x = self.relu(x)\n#         x = self.conv18(x)\n#         x = self.gn1(x)\n#         x = self.relu(x)\n#         x = self.conv19(x)\n#         x = torch.nn.functional.sigmoid(x) #since depth is between 0-1\n#         return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.009833Z","iopub.execute_input":"2024-12-08T13:37:13.010099Z","iopub.status.idle":"2024-12-08T13:37:13.021125Z","shell.execute_reply.started":"2024-12-08T13:37:13.010075Z","shell.execute_reply":"2024-12-08T13:37:13.020335Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# class CustomDepthEstimationModel(nn.Module):\n#     def __init__(self, input_channels=3, output_channels=1):\n#         super(CustomDepthEstimationModel, self).__init__()\n\n#         # Encoder: Custom DenseNet-like feature extractor (Dense blocks)\n#         self.encoder = nn.Sequential(\n#             self._dense_block(input_channels, 64, num_layers=6),\n#             self._dense_block(64, 128, num_layers=12),\n#             self._dense_block(128, 256, num_layers=24),\n#             self._dense_block(256, 512, num_layers=16)\n#         )\n\n#         # Decoder: Upsample and refine features progressively\n#         self.upconv1 = self._conv_block(512, 256)\n#         self.upconv2 = self._conv_block(256, 128)\n#         self.upconv3 = self._conv_block(128, 64)\n#         self.upconv4 = self._conv_block(64, 32)\n\n#         # Final Depth Map Prediction\n#         self.final_conv = nn.Conv2d(32, output_channels, kernel_size=(1, 1))  # Output single channel depth map\n\n#     def forward(self, x):\n#         # Encoder: Extract features\n#         x = self.encoder(x)\n\n#         # Decoder: Upsample and refine feature maps\n#         x = self.upconv1(x)\n#         x = self.upconv2(x)\n#         x = self.upconv3(x)\n#         x = self.upconv4(x)\n\n#         # Output: Depth map prediction\n#         depth_map = self.final_conv(x)\n#         return depth_map\n\n#     def _dense_block(self, in_channels, out_channels, num_layers):\n#         \"\"\"Create a dense block with multiple convolutional layers.\"\"\"\n#         layers = []\n#         for i in range(num_layers):\n#             layers.append(self._conv_block(in_channels, out_channels))\n#             in_channels = out_channels  # Each new layer takes all previous outputs\n\n#         return nn.Sequential(*layers)\n\n#     def _conv_block(self, in_channels, out_channels):\n#         \"\"\"Convolution block with ReLU activation, BatchNorm, and Dropout.\"\"\"\n#         block = nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n#             nn.ReLU(inplace=True),\n#             nn.BatchNorm2d(out_channels),\n#             nn.Dropout(0.2)\n#         )\n#         return block\n\n# # Instantiate the model\n# # model = DepthEstimationModel(input_channels=3, output_channels=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.022430Z","iopub.execute_input":"2024-12-08T13:37:13.022768Z","iopub.status.idle":"2024-12-08T13:37:13.036338Z","shell.execute_reply.started":"2024-12-08T13:37:13.022733Z","shell.execute_reply":"2024-12-08T13:37:13.035739Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class CustomDepthEstimationModel(nn.Module):\n    def __init__(self):\n        super(CustomDepthEstimationModel, self).__init__()\n        #channels: 3->64,\n        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n        #channels: 64->128\n        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n        self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n        #channels: 128->256\n        self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n        self.conv6 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n        self.maxpool3 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n        #channels: 256->512\n        self.conv7 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n        self.conv8 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n        self.maxpool4 = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n        #channels:512->1024\n        self.conv9 = nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, stride = 1, padding = 1)\n        self.conv10 = nn.Conv2d(in_channels = 1024, out_channels = 1024, kernel_size = 3, stride = 1, padding = 1)\n        self.upconv1 = nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size = 2, stride = 2)\n        #channels:1024->512\n        self.conv11 = nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n        self.conv12 = nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n        self.upconv2 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 2, stride = 2)\n        #channels: 512->256\n        self.conv13 = nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n        self.conv14 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n        self.upconv3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 2, stride = 2)\n        #channels:256->128\n        self.conv15 = nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n        self.conv16 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n        self.upconv4 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 2, stride = 2)\n        #channels:128->64\n        self.conv17 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n        self.conv18 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n        #channels:64->1\n        self.conv19 = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 1, stride = 1, padding = 0)\n\n        #relu: for non-linearity relu(x)=max(0,x)\n        self.relu = nn.ReLU()\n        #group_normalisation\n        self.gn1 = nn.GroupNorm(16, 64)\n        self.gn2 = nn.GroupNorm(16, 128)\n        self.gn3 = nn.GroupNorm(16, 256)\n        self.gn4 = nn.GroupNorm(16, 512)\n        self.gn5 = nn.GroupNorm(16, 1024)\n        #drop out layers; set the fraction of neurons to 0\n        self.dropout = nn.Dropout(0.5)\n        self.dropout1 = nn.Dropout(0.25)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.gn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.gn1(x)\n        x = self.relu(x)\n        out1 = x\n        x = self.maxpool1(x)\n        x = self.dropout1(x)\n        x = self.conv3(x)\n        x = self.gn2(x)\n        x = self.relu(x)\n        x = self.conv4(x)\n        x = self.gn2(x)\n        x = self.relu(x)\n        out2 = x\n        x = self.maxpool2(x)\n        x = self.dropout(x)\n        x = self.conv5(x)\n        x = self.gn3(x)\n        x = self.relu(x)\n        x = self.conv6(x)\n        x = self.gn3(x)\n        x = self.relu(x)\n        out3 = x\n        x = self.maxpool3(x)\n        x = self.dropout(x)\n        x = self.conv7(x)\n        x = self.gn4(x)\n        x = self.relu(x)\n        x = self.conv8(x)\n        x = self.gn4(x)\n        x = self.relu(x)\n        out4 = x\n        x = self.maxpool4(x)\n        x = self.dropout(x)\n        x = self.conv9(x)\n        x = self.gn5(x)\n        x = self.relu(x)\n        x = self.conv10(x)\n        x = self.gn5(x)\n        x = self.relu(x)\n        x = self.upconv1(x)\n        x = torch.cat((x, out4), 1)\n        x = self.dropout(x)\n        x = self.conv11(x)\n        x = self.gn4(x)\n        x = self.relu(x)\n        x = self.conv12(x)\n        x = self.gn4(x)\n        x = self.relu(x)\n        x = self.upconv2(x)\n        x = torch.cat((x, out3), 1)\n        x = self.dropout(x)\n        x = self.conv13(x)\n        x = self.gn3(x)\n        x = self.relu(x)\n        x = self.conv14(x)\n        x = self.gn3(x)\n        x = self.relu(x)\n        x = self.upconv3(x)\n        x = torch.cat((x, out2), 1)\n        x = self.dropout(x)\n        x = self.conv15(x)\n        x = self.gn2(x)\n        x = self.relu(x)\n        x = self.conv16(x)\n        x = self.gn2(x)\n        x = self.relu(x)\n        x = self.upconv4(x)\n        x = torch.cat((x, out1), 1)\n        x = self.dropout(x)\n        x = self.conv17(x)\n        x = self.gn1(x)\n        x = self.relu(x)\n        x = self.conv18(x)\n        x = self.gn1(x)\n        x = self.relu(x)\n        x = self.conv19(x)\n        x = torch.nn.functional.sigmoid(x) #since depth is between 0-1\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.037332Z","iopub.execute_input":"2024-12-08T13:37:13.037570Z","iopub.status.idle":"2024-12-08T13:37:13.058528Z","shell.execute_reply.started":"2024-12-08T13:37:13.037545Z","shell.execute_reply":"2024-12-08T13:37:13.057848Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# class CustomDepthEstimationModel(nn.Module):\n#     def __init__(self):\n#         super(CustomDepthEstimationModel, self).__init__()\n#         self.encoder = nn.Sequential(\n#             nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(kernel_size=2, stride=2)\n#         )\n#         self.decoder = nn.Sequential(\n#             nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n#             nn.ReLU(),\n#             nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n#         )\n\n#     def forward(self, x):\n#         x = self.encoder(x)\n#         x = self.decoder(x)\n#         return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.061472Z","iopub.execute_input":"2024-12-08T13:37:13.061819Z","iopub.status.idle":"2024-12-08T13:37:13.074803Z","shell.execute_reply.started":"2024-12-08T13:37:13.061794Z","shell.execute_reply":"2024-12-08T13:37:13.074055Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class CustomDepthEstimationModel(nn.Module):\n#     def __init__(self, dropout_rate=0.3):\n#         super(CustomDepthEstimationModel, self).__init__()\n\n#         # Encoder\n#         self.encoder_block1 = self._conv_block(3, 64, dropout_rate)  # Input channels = 3 (RGB)\n#         self.encoder_block2 = self._conv_block(64, 128, dropout_rate)\n#         self.encoder_block3 = self._conv_block(128, 256, dropout_rate)\n#         self.encoder_block4 = self._conv_block(256, 512, dropout_rate)\n\n#         # Bottleneck\n#         self.bottleneck = nn.Sequential(\n#             nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(512),\n#             nn.GELU(),\n#             nn.Dropout(p=dropout_rate)  # Regularization in bottleneck\n#         )\n\n#         # Decoder\n#         self.decoder_block1 = self._up_conv_block(512, 256, dropout_rate)\n#         self.decoder_block2 = self._up_conv_block(256, 128, dropout_rate)\n#         self.decoder_block3 = self._up_conv_block(128, 64, dropout_rate)\n#         self.decoder_block4 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)  # Output channel = 1 (depth map)\n\n#         # Skip Connections\n#         self.skip_conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n#         self.skip_conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n#         self.skip_conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n#     def _conv_block(self, in_channels, out_channels, dropout_rate):\n#         return nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.GELU(),\n#             nn.Dropout(p=dropout_rate),\n#             nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.GELU(),\n#             nn.Dropout(p=dropout_rate),\n#             nn.MaxPool2d(kernel_size=2, stride=2)\n#         )\n\n#     def _up_conv_block(self, in_channels, out_channels, dropout_rate):\n#         return nn.Sequential(\n#             nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0),\n#             nn.GELU(),\n#             nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.GELU(),\n#             nn.Dropout(p=dropout_rate)  # Add dropout for decoder too\n#         )\n\n#     def forward(self, x):\n#         # Encoder\n#         enc1 = self.encoder_block1(x)  # 64 channels\n#         enc2 = self.encoder_block2(enc1)  # 128 channels\n#         enc3 = self.encoder_block3(enc2)  # 256 channels\n#         enc4 = self.encoder_block4(enc3)  # 512 channels\n\n#         # Bottleneck\n#         bottleneck = self.bottleneck(enc4)\n\n#         # Decoder with skip connections\n#         dec1 = self.decoder_block1(bottleneck) + self.skip_conv3(enc3)\n#         dec2 = self.decoder_block2(dec1) + self.skip_conv2(enc2)\n#         dec3 = self.decoder_block3(dec2) + self.skip_conv1(enc1)\n#         dec4 = self.decoder_block4(dec3)  # Final depth map output\n\n#         return dec4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.076023Z","iopub.execute_input":"2024-12-08T13:37:13.076392Z","iopub.status.idle":"2024-12-08T13:37:13.089793Z","shell.execute_reply.started":"2024-12-08T13:37:13.076356Z","shell.execute_reply":"2024-12-08T13:37:13.088982Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class CustomDepthEstimationModel(nn.Module):\n#     def __init__(self):\n#         super(CustomDepthEstimationModel, self).__init__()\n\n#         # Encoder\n#         self.encoder_block1 = self._conv_block(3, 64)  # Input channels = 3 (RGB)\n#         self.encoder_block2 = self._conv_block(64, 128)\n#         self.encoder_block3 = self._conv_block(128, 256)\n#         self.encoder_block4 = self._conv_block(256, 512)\n\n#         # Decoder\n#         self.decoder_block1 = self._up_conv_block(512, 256)\n#         self.decoder_block2 = self._up_conv_block(256, 128)\n#         self.decoder_block3 = self._up_conv_block(128, 64)\n#         self.decoder_block4 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)  # Output channel = 1 (depth map)\n\n#         # Skip Connections\n#         self.skip_conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n#         self.skip_conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n#         self.skip_conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n#     def _conv_block(self, in_channels, out_channels):\n#         return nn.Sequential(\n#             nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.ReLU(),\n#             nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.ReLU(),\n#             nn.MaxPool2d(kernel_size=2, stride=2)\n#         )\n\n#     def _up_conv_block(self, in_channels, out_channels):\n#         return nn.Sequential(\n#             nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n#             nn.ReLU(),\n#             nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n#             nn.ReLU()\n#         )\n\n#     def forward(self, x):\n#         # Encoder\n#         enc1 = self.encoder_block1(x)  # 64 channels\n#         enc2 = self.encoder_block2(enc1)  # 128 channels\n#         enc3 = self.encoder_block3(enc2)  # 256 channels\n#         enc4 = self.encoder_block4(enc3)  # 512 channels\n\n#         # Decoder with skip connections\n#         dec1 = self.decoder_block1(enc4) + self.skip_conv3(enc3)  # Add skip connection\n#         dec2 = self.decoder_block2(dec1) + self.skip_conv2(enc2)\n#         dec3 = self.decoder_block3(dec2) + self.skip_conv1(enc1)\n#         dec4 = self.decoder_block4(dec3)  # Final depth map output\n\n#         return dec4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.090744Z","iopub.execute_input":"2024-12-08T13:37:13.090990Z","iopub.status.idle":"2024-12-08T13:37:13.106616Z","shell.execute_reply.started":"2024-12-08T13:37:13.090967Z","shell.execute_reply":"2024-12-08T13:37:13.105808Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def minmaxscaler(data):\n    return (data - np.min(data)) / (np.max(data) - np.min(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.107691Z","iopub.execute_input":"2024-12-08T13:37:13.107930Z","iopub.status.idle":"2024-12-08T13:37:13.122416Z","shell.execute_reply.started":"2024-12-08T13:37:13.107906Z","shell.execute_reply":"2024-12-08T13:37:13.121616Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def training_loop(model, criterion, optimizer, train_loader,epoch, epochs, device, scheduler=None):\n    \n    running_mse_loss = 0.0\n\n    model.train()\n\n    with tqdm(total=len(train_loader), desc=f'Epoch{epoch+1}/{epochs}', unit='batch') as tepoch:\n        for idx, (normal_img, depth_img) in enumerate(train_loader):  # Add 'idx' for batch index tracking\n            # Move images to device\n            normal_img = normal_img.to(device)\n            depth_img = depth_img.to(device)\n            # Zero the gradients\n            optimizer.zero_grad()\n            output = model(normal_img)\n            # loss = criterion(output, depth_img, normal_img)\n            loss = criterion(output, depth_img)\n            loss.backward()\n            optimizer.step()\n\n            running_mse_loss += loss.item()\n            \n            tepoch.set_postfix({'custom loss error': running_mse_loss})\n            tepoch.update(1)\n            \n        if scheduler:\n            scheduler.step()\n\n    return running_mse_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:32:14.007737Z","iopub.execute_input":"2024-12-08T17:32:14.008091Z","iopub.status.idle":"2024-12-08T17:32:14.014555Z","shell.execute_reply.started":"2024-12-08T17:32:14.008062Z","shell.execute_reply":"2024-12-08T17:32:14.013644Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Validation loop\ndef validation_loop(model, criterion, val_loader, device, epoch, epochs):\n    \n    running_val_loss = 0\n    model.eval()\n    with torch.no_grad():\n        with tqdm(total=len(val_loader), desc=f'Validation {epoch+1}/{epochs}', unit='batch') as tepoch:\n            for idx, (val_normal_img, val_depth_img) in enumerate(val_loader):\n                # Move images to device\n                val_normal_img = val_normal_img.to(device)\n                val_depth_img = val_depth_img.to(device)\n                # Forward pass\n                output = model(val_normal_img)\n                # Calculate loss\n                loss = criterion(output, val_depth_img, val_normal_img)\n                # loss = criterion(output, val_depth_img)\n                # Update running loss\n                running_val_loss += loss.item()\n                # average_val_loss = running_val_loss / (idx + 1)  # Calculate average loss so far\n                # Update tqdm bar with current and average loss\n                tepoch.set_postfix({'custom validation loss': running_val_loss})\n                tepoch.update(1)\n                \n                # Visualization Logic\n                # if idx %  == 0:\n                #     one_width = output.shape[3]\n                #     total_width = one_width * 3  # Create space for 3 images\n                #     new_val_image = Image.new('RGB', (total_width, output.shape[2]))\n                #     # Input Image.\n                #     new_val_image.paste(\n                #         Image.fromarray(np.uint8(minmaxscaler(val_normal_img[0].permute(1, 2, 0).detach().cpu().numpy()) * 255.)), (0, 0))\n                #     # Ground Truth (Labels).\n                #     new_val_image.paste(\n                #         Image.fromarray(np.uint8(minmaxscaler(val_depth_img[0].repeat(3, 1, 1).permute(1, 2, 0).detach().cpu().numpy()) * 255.)), (one_width, 0))\n                #     # Model's Output.\n                #     new_val_image.paste(\n                #         Image.fromarray(np.uint8(minmaxscaler(output[0].repeat(3, 1, 1).permute(1, 2, 0).detach().cpu().numpy()) * 255.)), (2 * one_width, 0))\n                #     # Save Combined Image.\n                #     new_val_image.save(f'combined_val_image_{epoch}_{idx}.png')\n\n    # return running_val_loss/len(val_loader)  \n    return running_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:32:16.277832Z","iopub.execute_input":"2024-12-08T17:32:16.278161Z","iopub.status.idle":"2024-12-08T17:32:16.284967Z","shell.execute_reply.started":"2024-12-08T17:32:16.278130Z","shell.execute_reply":"2024-12-08T17:32:16.284024Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# trianing model\ndef train_model(model, criterion, optimizer, train_loader, val_loader, epochs, device, scheduler=None):\n    best_val_loss = np.inf\n\n    for epoch in range(epochs):\n        # Training Loop\n        running_mse_loss = training_loop(model, criterion, optimizer, train_loader, epoch, epochs, device, scheduler)\n\n        # Validation Loop\n        val_loss = validation_loop(model, criterion, val_loader, device, epoch, epochs)\n        # Update best validation loss\n\n        # if scheduler:\n        #     scheduler.step(val_loss)  # Pass validation loss as the metric\n            \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), f'best_model_{epoch+41}.pth')\n            \n        print(f'Validation Loss: {val_loss}')\n        print(f'Mean squared erro: {best_val_loss}')\n\n    return model\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:27:57.066337Z","iopub.execute_input":"2024-12-08T17:27:57.067178Z","iopub.status.idle":"2024-12-08T17:27:57.072570Z","shell.execute_reply.started":"2024-12-08T17:27:57.067142Z","shell.execute_reply":"2024-12-08T17:27:57.071528Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"model = CustomDepthEstimationModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.158349Z","iopub.execute_input":"2024-12-08T13:37:13.158569Z","iopub.status.idle":"2024-12-08T13:37:13.479706Z","shell.execute_reply.started":"2024-12-08T13:37:13.158528Z","shell.execute_reply":"2024-12-08T13:37:13.478986Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = nn.DataParallel(model) \nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:13.480533Z","iopub.execute_input":"2024-12-08T13:37:13.480802Z","iopub.status.idle":"2024-12-08T13:37:13.813446Z","shell.execute_reply.started":"2024-12-08T13:37:13.480779Z","shell.execute_reply":"2024-12-08T13:37:13.812615Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): CustomDepthEstimationModel(\n    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv9): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv10): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n    (conv11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    (conv13): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    (conv15): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    (conv17): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv19): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n    (relu): ReLU()\n    (gn1): GroupNorm(16, 64, eps=1e-05, affine=True)\n    (gn2): GroupNorm(16, 128, eps=1e-05, affine=True)\n    (gn3): GroupNorm(16, 256, eps=1e-05, affine=True)\n    (gn4): GroupNorm(16, 512, eps=1e-05, affine=True)\n    (gn5): GroupNorm(16, 1024, eps=1e-05, affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (dropout1): Dropout(p=0.25, inplace=False)\n  )\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def scale_invariant_loss(y_pred, y_true):\n    diff = y_true - y_pred\n    return torch.mean(diff*2) - 0.5 * torch.mean(diff)*2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:28:08.450737Z","iopub.execute_input":"2024-12-08T17:28:08.451077Z","iopub.status.idle":"2024-12-08T17:28:08.455658Z","shell.execute_reply.started":"2024-12-08T17:28:08.451046Z","shell.execute_reply":"2024-12-08T17:28:08.454646Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def edge_aware_smoothness_loss(pred, image):\n    dx_pred = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n    dy_pred = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n    dx_img = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:]), dim=1, keepdim=True)\n    dy_img = torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]), dim=1, keepdim=True)\n    loss = torch.mean(dx_pred * torch.exp(-dx_img)) + torch.mean(dy_pred * torch.exp(-dy_img))\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:28:10.986494Z","iopub.execute_input":"2024-12-08T17:28:10.987108Z","iopub.status.idle":"2024-12-08T17:28:10.992486Z","shell.execute_reply.started":"2024-12-08T17:28:10.987073Z","shell.execute_reply":"2024-12-08T17:28:10.991642Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# def combined_loss(y_pred, y_true, image):\ndef combined_loss(y_pred, y_true):\n    mse_loss = nn.MSELoss()(y_pred, y_true)\n    scale_inv = scale_invariant_loss(y_pred, y_true)\n    edge_loss = edge_aware_smoothness_loss(y_pred, image)\n    return mse_loss + 0.1 * scale_inv + 0.1 * edge_loss\n    # return mse_loss + 0.1*scale_inv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:32:32.957478Z","iopub.execute_input":"2024-12-08T17:32:32.958280Z","iopub.status.idle":"2024-12-08T17:32:32.962482Z","shell.execute_reply.started":"2024-12-08T17:32:32.958246Z","shell.execute_reply":"2024-12-08T17:32:32.961504Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:32:36.351823Z","iopub.execute_input":"2024-12-08T17:32:36.352155Z","iopub.status.idle":"2024-12-08T17:32:36.356711Z","shell.execute_reply.started":"2024-12-08T17:32:36.352127Z","shell.execute_reply":"2024-12-08T17:32:36.355642Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:32:39.076574Z","iopub.execute_input":"2024-12-08T17:32:39.076961Z","iopub.status.idle":"2024-12-08T17:32:39.081013Z","shell.execute_reply.started":"2024-12-08T17:32:39.076929Z","shell.execute_reply":"2024-12-08T17:32:39.080093Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"num_epochs = 3\n# criterion = nn.MSELoss()\ncrieterion = combined_loss\n\noptimizer = optim.AdamW(model.parameters(), lr=0.0001)\n\n# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.0001)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1, verbose=True)\n\nmodel = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, device, scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:30:05.191745Z","iopub.execute_input":"2024-12-08T16:30:05.192419Z","iopub.status.idle":"2024-12-08T17:11:34.902724Z","shell.execute_reply.started":"2024-12-08T16:30:05.192387Z","shell.execute_reply":"2024-12-08T17:11:34.901776Z"}},"outputs":[{"name":"stderr","text":"Epoch1/10: 100%|██████████| 209/209 [04:26<00:00,  1.27s/batch, Mean Squared Error=1.22] \nValidation 1/10: 100%|██████████| 27/27 [00:14<00:00,  1.86batch/s, Mean Squared Error=0.163] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.16323752515017986\nMean squared erro: 0.16323752515017986\n","output_type":"stream"},{"name":"stderr","text":"Epoch2/10: 100%|██████████| 209/209 [03:50<00:00,  1.10s/batch, Mean Squared Error=1.17] \nValidation 2/10: 100%|██████████| 27/27 [00:12<00:00,  2.12batch/s, Mean Squared Error=0.188] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.18791671842336655\nMean squared erro: 0.16323752515017986\n","output_type":"stream"},{"name":"stderr","text":"Epoch3/10: 100%|██████████| 209/209 [03:59<00:00,  1.15s/batch, Mean Squared Error=1.17] \nValidation 3/10: 100%|██████████| 27/27 [00:13<00:00,  2.06batch/s, Mean Squared Error=0.168] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1683440306223929\nMean squared erro: 0.16323752515017986\n","output_type":"stream"},{"name":"stderr","text":"Epoch4/10: 100%|██████████| 209/209 [03:56<00:00,  1.13s/batch, Mean Squared Error=1.14] \nValidation 4/10: 100%|██████████| 27/27 [00:12<00:00,  2.12batch/s, Mean Squared Error=0.161] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.16068708570674062\nMean squared erro: 0.16068708570674062\n","output_type":"stream"},{"name":"stderr","text":"Epoch5/10: 100%|██████████| 209/209 [03:51<00:00,  1.11s/batch, Mean Squared Error=1.11] \nValidation 5/10: 100%|██████████| 27/27 [00:12<00:00,  2.08batch/s, Mean Squared Error=0.17]  \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.16976424679160118\nMean squared erro: 0.16068708570674062\n","output_type":"stream"},{"name":"stderr","text":"Epoch6/10: 100%|██████████| 209/209 [03:50<00:00,  1.10s/batch, Mean Squared Error=1.12] \nValidation 6/10: 100%|██████████| 27/27 [00:12<00:00,  2.09batch/s, Mean Squared Error=0.154] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.15384415700100362\nMean squared erro: 0.15384415700100362\n","output_type":"stream"},{"name":"stderr","text":"Epoch7/10: 100%|██████████| 209/209 [03:52<00:00,  1.11s/batch, Mean Squared Error=1.09] \nValidation 7/10: 100%|██████████| 27/27 [00:12<00:00,  2.08batch/s, Mean Squared Error=0.149] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.14911336870864034\nMean squared erro: 0.14911336870864034\n","output_type":"stream"},{"name":"stderr","text":"Epoch8/10: 100%|██████████| 209/209 [03:50<00:00,  1.10s/batch, Mean Squared Error=1.05] \nValidation 8/10: 100%|██████████| 27/27 [00:12<00:00,  2.11batch/s, Mean Squared Error=0.15]  \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.15009353985078633\nMean squared erro: 0.14911336870864034\n","output_type":"stream"},{"name":"stderr","text":"Epoch9/10: 100%|██████████| 209/209 [03:49<00:00,  1.10s/batch, Mean Squared Error=1.05] \nValidation 9/10: 100%|██████████| 27/27 [00:12<00:00,  2.12batch/s, Mean Squared Error=0.157] \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.15718174213543534\nMean squared erro: 0.14911336870864034\n","output_type":"stream"},{"name":"stderr","text":"Epoch10/10: 100%|██████████| 209/209 [03:51<00:00,  1.11s/batch, Mean Squared Error=1.03] \nValidation 10/10: 100%|██████████| 27/27 [00:12<00:00,  2.11batch/s, Mean Squared Error=0.152] ","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.1516664857044816\nMean squared erro: 0.14911336870864034\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"os.makedirs('/kaggle/working/outputs', exist_ok = True)\npath = '/kaggle/working/outputs/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:24:03.711860Z","iopub.execute_input":"2024-12-08T16:24:03.712127Z","iopub.status.idle":"2024-12-08T16:24:03.716205Z","shell.execute_reply.started":"2024-12-08T16:24:03.712100Z","shell.execute_reply":"2024-12-08T16:24:03.715305Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def minmaxscaler_prediction(tensor):\n        \"\"\"\n        Scale a PyTorch tensor to the range [0, 1].\n        \"\"\"\n        return (tensor - tensor.min()) / (tensor.max() - tensor.min())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:24:03.717113Z","iopub.execute_input":"2024-12-08T16:24:03.717389Z","iopub.status.idle":"2024-12-08T16:24:03.729875Z","shell.execute_reply.started":"2024-12-08T16:24:03.717359Z","shell.execute_reply":"2024-12-08T16:24:03.728885Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# # # test prediction code\ndef test_prediction(model, test_loader, device, path):\n    model.eval()\n    with torch.no_grad():\n        for test_img, names in test_loader:\n            test_img = test_img.to(device)\n            output = model(test_img)\n\n            # print(output.shape)\n            for idx in range(output.shape[0]):\n                # Scale the output tensor to [0, 1]\n                scaled_output = minmaxscaler_prediction(output[idx].detach().cpu().squeeze())\n    \n                # Convert the scaled output to a NumPy array and scale to 0-255\n                scaled_output = (scaled_output.numpy() * 255.0).astype(np.uint8)\n    \n                # Create an image from the scaled output\n                out_image = Image.fromarray(scaled_output, mode='L')\n                \n                \n                # Save the image with the given name\n                out_path = os.path.join(path, f\"{names[idx]}\")\n                out_image.save(out_path)\n            \n            # save_image(output, f'output_{idx}.png')\n            # return output\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:24:03.730918Z","iopub.execute_input":"2024-12-08T16:24:03.731200Z","iopub.status.idle":"2024-12-08T16:24:03.742521Z","shell.execute_reply.started":"2024-12-08T16:24:03.731171Z","shell.execute_reply":"2024-12-08T16:24:03.741632Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#  loading the best model\n# model.load_state_dict(torch.load('/kaggle/working/best_model_6.pth'))\n# model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:16:41.617503Z","iopub.execute_input":"2024-12-08T17:16:41.617880Z","iopub.status.idle":"2024-12-08T17:16:41.693519Z","shell.execute_reply.started":"2024-12-08T17:16:41.617850Z","shell.execute_reply":"2024-12-08T17:16:41.692631Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): CustomDepthEstimationModel(\n    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv9): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv10): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n    (conv11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    (conv13): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    (conv15): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    (conv17): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (conv19): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n    (relu): ReLU()\n    (gn1): GroupNorm(16, 64, eps=1e-05, affine=True)\n    (gn2): GroupNorm(16, 128, eps=1e-05, affine=True)\n    (gn3): GroupNorm(16, 256, eps=1e-05, affine=True)\n    (gn4): GroupNorm(16, 512, eps=1e-05, affine=True)\n    (gn5): GroupNorm(16, 1024, eps=1e-05, affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (dropout1): Dropout(p=0.25, inplace=False)\n  )\n)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"test_prediction(model, test_loader, device, path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:16:49.190699Z","iopub.execute_input":"2024-12-08T17:16:49.191362Z","iopub.status.idle":"2024-12-08T17:17:11.473879Z","shell.execute_reply.started":"2024-12-08T17:16:49.191332Z","shell.execute_reply":"2024-12-08T17:17:11.473101Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\n\ndef images_to_csv_with_metadata(image_folder, output_csv):\n    # Initialize an empty list to store image data and metadata\n    data = []\n\n    # Loop through all images in the folder\n    for idx, filename in enumerate(sorted(os.listdir(image_folder))):\n        if filename.endswith(\".png\"):\n            filepath = os.path.join(image_folder, filename)\n            # Read the image\n            image = cv2.imread(filepath, cv2.IMREAD_UNCHANGED)\n            image = cv2.resize(image, (128, 128))\n            image = image / 255.\n            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-6)\n            image = np.uint8(image * 255.)\n            # Flatten the image into a 1D array\n            image_flat = image.flatten()\n            # Add ID, ImageID (filename), and pixel values\n            row = [idx, filename] + image_flat.tolist()\n            data.append(row)\n    \n    # Create a DataFrame\n    num_columns = len(data[0]) - 2 if data else 0\n    column_names = [\"id\", \"ImageID\"] + [indx for indx in range(num_columns)]\n    df = pd.DataFrame(data, columns=column_names)\n\n    # Save to CSV\n    df.to_csv(output_csv, index=False)\n\n# Paths for prediction and ground truth images\npredictions_folder = \"/kaggle/working/outputs\"\n\n# Output CSV paths\npredictions_csv = \"predictions.csv\"\n\n# Convert prediction images to CSV\nimages_to_csv_with_metadata(predictions_folder, predictions_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T17:18:06.516059Z","iopub.execute_input":"2024-12-08T17:18:06.517170Z","iopub.status.idle":"2024-12-08T17:18:16.302377Z","shell.execute_reply.started":"2024-12-08T17:18:06.517123Z","shell.execute_reply":"2024-12-08T17:18:16.301457Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}